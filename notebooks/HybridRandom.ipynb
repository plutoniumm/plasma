{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current problem we're facing is\n",
    "\n",
    "> The simulations run too slow. An average iteration in `Net` below for a classical layer takes ~10ns whereas for a quantum layer it takes ~0.5s, that too parallelized to (8/16 threads)\n",
    "\n",
    "This is most likely since most of Qiskit is written in Python and not in C-family/Fortran top to bottom. In fact the only C layer in all of Qiskit is the `Aer` package which is why it takes 0.5s and not something like 15s.\n",
    "\n",
    "There is no way I know of so far of circumventing this problem. the following are a few considerations:\n",
    "- Using **TorchQuantum**: TorchQuantum does not support Apple Silicon installations (i am using an M2 laptop). The issue is tracked [here](https://github.com/mit-han-lab/torchquantum/issues/98)\n",
    "- Use the **GPU**: The current Benchmark for M2 CPU is faster than the Colab GPU\n",
    "- Using **Apple Silicon** to its full extent: While pytorch is already ready for it, Qiskit is not. The issue is tracked [here](https://github.com/Qiskit/qiskit-aer/issues/1762). With full Apple Silicon support, we can use the M2 to basically as much power as the same order of magnitude as Titan\n",
    "- Using a **Different Algorithm**: See [QCNN.ipynb](./QCNN.ipynb)\n",
    "- Using **Runtime Primitives**: The `EstimatorQNN` class actually returns the energy levels measured in various ways. The circuit is not learning when using those/I don't know how to use it (since I can't find a lot of examples online)\n",
    "- Using `TorchConnector`: There is no speed/learning benefit. Under the hood it uses the same `EstimatorQNN` class\n",
    "\n",
    "### The situation\n",
    "We know for a fact this model works because under various configurations we are seeing learning happening. For $(0,1)$ case it is almost a perfect classifier. For $(0,1,3,6)$ case for small training size it is random but as the trainset becomes larger it starts becoming more and more better than random (I was able to reach ~40%, perfectly random is ~25%)\n",
    "\n",
    "It stands to reason if Qiskit were faster we would see the same thing happening for the full MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_aer.primitives import Estimator\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit import QuantumCircuit\n",
    "from time import time\n",
    "\n",
    "from utils import gtt\n",
    "\n",
    "n_train = 100\n",
    "n_test = 10\n",
    "qubits = 13\n",
    "shots = 1024\n",
    "\n",
    "# train_loader, test_loader = gtt(n_train, [i for i in range(10)])\n",
    "train_loader, test_loader = gtt(n_train, [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NC4 from I, X, Y, Z\n",
    "paulis = [\"I\", \"X\", \"Y\", \"Z\"]\n",
    "def select(n):\n",
    "    if n == 0:\n",
    "        return [\"\"]\n",
    "    if n == 1:\n",
    "        return [\"I\", \"X\", \"Y\", \"Z\"]\n",
    "    return [a+b for a in select(n-1) for b in paulis]\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, qubits):\n",
    "        self.backend = Estimator()\n",
    "        possible = select(5 if qubits > 5 else qubits)\n",
    "        # TODO: select properly\n",
    "        possible = np.random.choice(possible, qubits)\n",
    "        if qubits > 5:\n",
    "            possible = [\"I\"*(qubits-5)+p for p in possible]\n",
    "        self.obs = [self.op(p) for p in possible]\n",
    "\n",
    "        circ = QuantumCircuit(qubits)\n",
    "        for i in range(qubits):\n",
    "            circ.h(i)\n",
    "            t = Parameter('t'+str(i))\n",
    "            circ.cx(i, (i+1)%qubits)\n",
    "            circ.rx(t, i)\n",
    "        circ.measure_all()\n",
    "        self.circuit = circ\n",
    "\n",
    "    def op(self, p):\n",
    "        return SparsePauliOp.from_list([(p, 1)])\n",
    "\n",
    "    def run(self, inputs): # Runs a circuit\n",
    "        qc = self.circuit.assign_parameters(inputs)\n",
    "        result = self.backend.run(\n",
    "            [qc]*len(self.obs),\n",
    "            self.obs,\n",
    "            shots=shots,\n",
    "        ).result()\n",
    "        return torch.tensor(result.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridFunction(Function):\n",
    "    def forward(ctx, input, quantum_circuit):\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.quantum_circuit = quantum_circuit\n",
    "\n",
    "        results = []\n",
    "        for i in range(len(input)):\n",
    "            expz = ctx.quantum_circuit.run(input[i].tolist())\n",
    "            results.append(torch.tensor(np.array([expz])))\n",
    "\n",
    "        return torch.stack(results).squeeze(1)\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs = np.array(ctx.saved_tensors[0])\n",
    "        shift = np.ones(inputs.shape) * np.pi/2\n",
    "\n",
    "        shiftr = inputs + shift\n",
    "        shiftl = inputs - shift\n",
    "\n",
    "        gradients = []\n",
    "        for i in range(len(inputs)):\n",
    "            expr = ctx.quantum_circuit.run(shiftr[i])\n",
    "            expl = ctx.quantum_circuit.run(shiftl[i])\n",
    "\n",
    "            gradient = torch.tensor(expr - expl)\n",
    "            gradients.append(gradient)\n",
    "        # endfor\n",
    "\n",
    "        gradients = torch.stack(gradients).squeeze(1)\n",
    "        return gradients * grad_output.float(), None, None\n",
    "\n",
    "class Hybrid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Hybrid, self).__init__()\n",
    "        self.quantum_circuit = Generator(13)\n",
    "    # end\n",
    "\n",
    "    def forward(self, input):\n",
    "        return HybridFunction.apply(input, self.quantum_circuit)\n",
    "    # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/y75db3512zv_8r60rn4pb82h0000gn/T/ipykernel_75047/2635976318.py:13: DeprecationWarning: Option approximation=False is deprecated as of qiskit-aer 0.13. It will be removed no earlier than 3 months after the release date. Instead, use BackendEstmator from qiskit.primitives.\n",
      "  self.backend = Estimator()\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "perfs = []\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=4)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=4)\n",
    "        out_conv1 = F.max_pool2d(self.conv1(torch.rand(1,1,28,28)), 2);\n",
    "        out_conv2 = F.max_pool2d(self.conv2(out_conv1), 2)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(out_conv2.view(1,-1).shape[1], qubits)\n",
    "        self.hybrid = Hybrid()\n",
    "        out_hybrid = self.hybrid(self.fc1(out_conv2.view(1,-1)))\n",
    "        self.fc2 = nn.Linear(out_hybrid.shape[1], 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer = []\n",
    "        s = perf_counter()\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x) # NO RELU, pass as-is\n",
    "        x = self.hybrid(x).type(torch.FloatTensor)\n",
    "        x = self.fc2(x)\n",
    "        perfs.append(layer)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(np_mat):\n",
    "  sums = np_mat.sum(axis=1)\n",
    "  return np_mat / sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/y75db3512zv_8r60rn4pb82h0000gn/T/ipykernel_75047/4015201048.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gradient = torch.tensor(expr - expl)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Itr Time: 13.3s x 5 itrs = 1.1min\n",
      "Perfs: [2.232e-03 2.498e+00 2.500e+00]\n",
      "Training [5%]\tLoss: 2.2713\n",
      "Avg Itr Time: 12.8s x 5 itrs = 1.1min\n",
      "Perfs: [4.036e-03 5.000e+00 5.000e+00]\n",
      "Training [10%]\tLoss: 2.2644\n",
      "Avg Itr Time: 12.0s x 5 itrs = 1.0min\n",
      "Perfs: [0.007904 7.5      7.5     ]\n",
      "Training [15%]\tLoss: 2.2568\n",
      "Avg Itr Time: 12.5s x 5 itrs = 1.0min\n",
      "Perfs: [ 0.010414 10.       10.      ]\n",
      "Training [20%]\tLoss: 2.2522\n",
      "Avg Itr Time: 12.3s x 5 itrs = 1.0min\n",
      "Perfs: [ 0.0125 12.5    12.5   ]\n",
      "Training [25%]\tLoss: 2.2394\n",
      "Avg Itr Time: 13.9s x 5 itrs = 1.2min\n",
      "Perfs: [1.467e-02 1.500e+01 1.500e+01]\n",
      "Training [30%]\tLoss: 2.2326\n",
      "Avg Itr Time: 12.5s x 5 itrs = 1.0min\n",
      "Perfs: [ 0.02032 17.5     17.5    ]\n",
      "Training [35%]\tLoss: 2.2256\n",
      "Avg Itr Time: 13.9s x 5 itrs = 1.2min\n",
      "Perfs: [ 0.02716 20.      20.     ]\n",
      "Training [40%]\tLoss: 2.2163\n",
      "Avg Itr Time: 10.7s x 5 itrs = 0.9min\n",
      "Perfs: [ 0.0293 22.5    22.5   ]\n",
      "Training [45%]\tLoss: 2.2059\n",
      "Avg Itr Time: 197.8s x 5 itrs = 16.5min\n",
      "Perfs: [ 0.0317 25.     25.    ]\n",
      "Training [50%]\tLoss: 2.1982\n",
      "Avg Itr Time: 198.3s x 5 itrs = 16.5min\n",
      "Perfs: [ 0.03378 27.5     27.5    ]\n",
      "Training [55%]\tLoss: 2.1893\n",
      "Avg Itr Time: 195.8s x 5 itrs = 16.3min\n",
      "Perfs: [ 0.0362 30.     30.    ]\n",
      "Training [60%]\tLoss: 2.1796\n",
      "Avg Itr Time: 385.5s x 5 itrs = 32.1min\n",
      "Perfs: [ 0.038 32.5   32.5  ]\n",
      "Training [65%]\tLoss: 2.1711\n",
      "Avg Itr Time: 195.7s x 5 itrs = 16.3min\n",
      "Perfs: [ 0.04025 35.      35.     ]\n",
      "Training [70%]\tLoss: 2.1609\n",
      "Avg Itr Time: 50.7s x 5 itrs = 4.2min\n",
      "Perfs: [ 0.04254 37.5     37.5    ]\n",
      "Training [75%]\tLoss: 2.1496\n",
      "Avg Itr Time: 195.6s x 5 itrs = 16.3min\n",
      "Perfs: [ 0.0441 40.     40.    ]\n",
      "Training [80%]\tLoss: 2.1401\n",
      "Avg Itr Time: 219.8s x 5 itrs = 18.3min\n",
      "Perfs: [ 0.0454 42.5    42.5   ]\n",
      "Training [85%]\tLoss: 2.1316\n",
      "Avg Itr Time: 195.3s x 5 itrs = 16.3min\n",
      "Perfs: [ 0.04813 45.      45.     ]\n",
      "Training [90%]\tLoss: 2.1234\n",
      "Avg Itr Time: 195.3s x 5 itrs = 16.3min\n",
      "Perfs: [ 0.05017 47.5     47.5    ]\n",
      "Training [95%]\tLoss: 2.1116\n",
      "Avg Itr Time: 144.7s x 5 itrs = 12.1min\n",
      "Perfs: [ 0.05237 50.      50.     ]\n",
      "Training [100%]\tLoss: 2.1030\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20 # for now 20\n",
    "loss_list = [3]\n",
    "\n",
    "model.train()\n",
    "\n",
    "targets = []\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    times = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        now = time()\n",
    "        # optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        targets.append(target)\n",
    "\n",
    "        loss = loss_func(output, target) # Loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        times.append(time() - now)\n",
    "\n",
    "    print(f\"Avg Itr Time: {np.round(np.average(times),1)}s x {len(times)} itrs = {np.round(np.sum(times)/60,1)}min\")\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "\n",
    "    # process perfs\n",
    "    perfs2 = normalise(np.array(perfs).astype(np.float16))\n",
    "    colsums = perfs2.sum(axis=0)\n",
    "    print(f\"Perfs: {colsums}\")\n",
    "    print(f'Training [{100. * (epoch + 1) / epochs:.0f}%]\\tLoss: {loss_list[-1]:.4f}')\n",
    "\n",
    "    diff = np.abs(loss_list[-1] - loss_list[-2]) /loss_list[-1]\n",
    "\n",
    "    if diff <= 0.001:\n",
    "        break;\n",
    "    # endfor\n",
    "# endfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: 2.4200\n",
      "\tAccuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "with torch.no_grad(): # Don't compute gradients\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader): # Loop over the test set\n",
    "        output = model(data)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = loss_func(output, target)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    print('Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n",
    "        sum(total_loss) / len(total_loss),\n",
    "        correct / len(test_loader) * 100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = model(data)\n",
    "        print(output)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title('Predicted {}'.format(pred.item()))\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
