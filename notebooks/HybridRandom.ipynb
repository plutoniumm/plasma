{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying all of MNIST\n",
    "\n",
    "Current SOA does only 2 digits, `IBM=(0,1)`, `Google=(3,6)`. We shall attempt to do all 10 digits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This codebook is created for the purpose of trying to classify the full MNIST dataset using a hybrid quantum-classical neural network. The codebook is divided into 3 parts: \n",
    "\n",
    "0. Python imports\n",
    "1. Data preparation \n",
    "2. Quantum neural network \n",
    "3. Classical neural network \n",
    "4. Hybrid quantum-classical neural network \n",
    "5. Results "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current problem we're facing is\n",
    "\n",
    "> The simulations run too slow. An average iteration in `Net` below for a classical layer takes ~10ns whereas for a quantum layer it takes ~0.5s, that too parallelized to (8/16 threads)\n",
    "\n",
    "This is most likely since most of Qiskit is written in Python and not in C-family/Fortran top to bottom. In fact the only C layer in all of Qiskit is the `Aer` package which is why it takes 0.5s and not something like 15s.\n",
    "\n",
    "There is no way I know of so far of circumventing this problem. the following are a few considerations:\n",
    "- Using **TorchQuantum**: TorchQuantum does not support Apple Silicon installations (i am using an M2 laptop). The issue is tracked [here](https://github.com/mit-han-lab/torchquantum/issues/98)\n",
    "- Use the **GPU**: The current Benchmark for M2 CPU is faster than the Colab GPU\n",
    "- Using **Apple Silicon** to its full extent: While pytorch is already ready for it, Qiskit is not. The issue is tracked [here](https://github.com/Qiskit/qiskit-aer/issues/1762). With full Apple Silicon support, we can use the M2 to basically as much power as the same order of magnitude as Titan\n",
    "- Using a **Different Algorithm**: See [QCNN.ipynb](./QCNN.ipynb)\n",
    "- Using **Runtime Primitives**: The `EstimatorQNN` class actually returns the energy levels measured in various ways. The circuit is not learning when using those/I don't know how to use it (since I can't find a lot of examples online)\n",
    "- Using `TorchConnector`: There is no speed/learning benefit. Under the hood it uses the same `EstimatorQNN` class\n",
    "\n",
    "### The situation\n",
    "We know for a fact this model works because under various configurations we are seeing learning happening. For $(0,1)$ case it is almost a perfect classifier. For $(0,1,3,6)$ case for small training size it is random but as the trainset becomes larger it starts becoming more and more better than random (I was able to reach ~40%, perfectly random is ~25%)\n",
    "\n",
    "It stands to reason if Qiskit were faster we would see the same thing happening for the full MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqiskit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantum_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparsePauliOp\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gtt, make_filt\n",
      "File \u001b[0;32m~/Documents/GitHub.nosync/IITM/plasma/notebooks/utils.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array, append, where, \u001b[38;5;28msum\u001b[39m, sqrt\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqiskit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m algorithm_globals\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import qiskit\n",
    "from qiskit.visualization import *\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time as t\n",
    "\n",
    "from utils import gtt, make_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1000;\n",
    "n_test = 100;\n",
    "qubits = 13\n",
    "shots = 256\n",
    "threads = 8\n",
    "\n",
    "train_loader, test_loader = gtt(n_train, [i for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweight(dic):\n",
    "    return [k for k, v in dic.items() for i in range(v)]\n",
    "\n",
    "def get_probabilities(results):\n",
    "\n",
    "    if isinstance(results, dict):\n",
    "        results = [results]\n",
    "\n",
    "    probabilities = []\n",
    "    for result in results:\n",
    "        arr = np.mean([list(map(int, x)) for x in unweight(result)], axis=0)\n",
    "        probabilities.append(arr)\n",
    "\n",
    "    return probabilities[0] if len(probabilities)==1 else probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```js\n",
    "\"IN IMAGE\" = Conv = Conv = DropOut = Linear\n",
    "||\n",
    "Quantum\n",
    "||\n",
    "Linear = \"OUTPIT\"\n",
    "```\n",
    "\n",
    "This is based off of the Standard Hybrid Tutorial on the Qiskit docs to be used with PyTorch. The general idea is to use\n",
    "\n",
    "2 Convolutional Layers &rarr; Linear as a link &rarr; Quantum Layer &rarr; Linear as a link &rarr; 10 outputs for 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits):\n",
    "        self.threads = threads; # set number of || threads\n",
    "        circs = []\n",
    "        for i in range(threads): # Creates 8 || identical circuits\n",
    "            circ = qiskit.QuantumCircuit(qubits);\n",
    "            all_qubits = [i for i in range(qubits)]\n",
    "            circ.h(all_qubits)\n",
    "            # Parametrisation\n",
    "            params = [qiskit.circuit.Parameter('theta')]\n",
    "            circ.rx(params[0], all_qubits)\n",
    "            # run\n",
    "            circ.measure_all()\n",
    "\n",
    "            param_dict = {param: np.random.random() for param in params}\n",
    "            bound_circuit = circ.assign_parameters(parameters = param_dict)\n",
    "\n",
    "            circs.append(bound_circuit);\n",
    "\n",
    "        self.circuits = circs;\n",
    "\n",
    "    def runner(self, circuit): # Runs a circuit\n",
    "        backend = AerSimulator()\n",
    "\n",
    "        result = qiskit.execute(circuit, backend, shots=int(shots/threads)).result()\n",
    "        result = get_probabilities(result.get_counts(circuit))\n",
    "        return result\n",
    "\n",
    "    def run(self, inputs): # || execution of circuits\n",
    "        reses = None;\n",
    "        with ThreadPoolExecutor(max_workers=len(self.circuits)) as executor:\n",
    "            reses = list(executor.map(self.runner, self.circuits))\n",
    "\n",
    "        return np.average(reses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridFunction(Function): # more or less the same as in the tutorial\n",
    "    \"\"\" Hybrid quantum - classical function definition \"\"\"\n",
    "\n",
    "    @staticmethod  # Note: the @staticmethod decorator is not strictly necessary here\n",
    "    def forward(ctx, input, quantum_circuit):\n",
    "        \"\"\" Forward pass computation \"\"\"\n",
    "        ctx.shift = np.pi / 2;  # Store the shift value for the backward pass\n",
    "        # Store the quantum circuit for the backward pass\n",
    "        ctx.quantum_circuit = quantum_circuit\n",
    "\n",
    "        results = [];\n",
    "        for i in range(len(input)):\n",
    "            expectation_z = ctx.quantum_circuit.run(input[i].tolist())\n",
    "            results.append(torch.tensor(np.array([expectation_z])))\n",
    "\n",
    "        # Save the input and the result for the backward pass\n",
    "        results = torch.stack(results).squeeze(1)\n",
    "        ctx.save_for_backward(input, results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\" Backward pass computation \"\"\"\n",
    "        input, expectation_z = ctx.saved_tensors  # Load the saved tensors\n",
    "        # Convert the input to a numpy array\n",
    "        input_list = np.array(input.tolist())\n",
    "\n",
    "        shift_right = input_list + np.ones(input_list.shape) * ctx.shift # Shift right\n",
    "        shift_left = input_list - np.ones(input_list.shape) * ctx.shift # Shift left\n",
    "\n",
    "        gradients = []\n",
    "        for i in range(len(input_list)):\n",
    "            expectation_right = ctx.quantum_circuit.run(shift_right[i]) # Run the quantum circuit for the right shift\n",
    "            expectation_left = ctx.quantum_circuit.run(shift_left[i]) # Run the quantum circuit for the left shift\n",
    "\n",
    "            gradient = torch.tensor(np.array([expectation_right])) - \\\n",
    "                torch.tensor(np.array([expectation_left])) # Compute the gradient\n",
    "            gradients.append(gradient)\n",
    "\n",
    "        # gradients = np.array([gradients]).T\n",
    "        gradients = torch.stack(gradients).squeeze(1)\n",
    "        return gradients * grad_output.float(), None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid(nn.Module): # more or less the same as in the tutorial\n",
    "    \"\"\" Hybrid quantum - classical layer definition \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Hybrid, self).__init__()\n",
    "        self.quantum_circuit = QuantumCircuit(10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return HybridFunction.apply(input, self.quantum_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # the actual neural net as mentioned in the tutorial\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=4)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=4)\n",
    "        out_conv1 = F.max_pool2d(self.conv1(torch.rand(1,1,28,28)), 2);\n",
    "        out_conv2 = F.max_pool2d(self.conv2(out_conv1), 2)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(out_conv2.view(1,-1).shape[1], qubits)\n",
    "        self.hybrid = Hybrid()\n",
    "        out_hybrid = self.hybrid(torch.rand(qubits,qubits))\n",
    "        self.fc2 = nn.Linear(out_hybrid.shape[1], 10)\n",
    "\n",
    "#         each conv reduces size, the more the better so that we ensure that the quantum does the heavy lifting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x) # We don't relu this to prevent learning, we pass as-is to QC\n",
    "        x = self.hybrid(x).type(torch.FloatTensor)\n",
    "        x = self.fc2(x)\n",
    "        return x;\n",
    "\n",
    "model = Net();\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer\n",
    "loss_func = nn.CrossEntropyLoss() # Cross entropy loss since we're doing classification\n",
    "\n",
    "epochs = 20 # for now 20\n",
    "loss_list = [3] # we need to intialize this to something, 3 is arbitrary\n",
    "\n",
    "model.train() # Set the model to training mode\n",
    "\n",
    "outputs = []\n",
    "targets = []\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    times = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        now = t.time()\n",
    "        optimizer.zero_grad() # Clear the gradients\n",
    "        # Forward pass\n",
    "        output = model(data) # Forward pass\n",
    "\n",
    "        outputs.append(torch.argmax(output))\n",
    "        targets.append(target)\n",
    "\n",
    "        loss = loss_func(output, target) # Loss\n",
    "\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step() # Optimize the weights\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        times.append(t.time() - now)\n",
    "\n",
    "    print(f\"Avg Itr Time: {np.round(np.average(times),1)}s x {len(times)} itrs = {np.round(np.sum(times)/60,1)}min\")\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "\n",
    "    diff = np.abs(loss_list[-1] - loss_list[-2]) /loss_list[-1];\n",
    "    if diff <= 0.001: # Early stopping criterial loss diff = 0.1%\n",
    "        break;\n",
    "\n",
    "    print(f'Training [{100. * (epoch + 1) / epochs:.0f}%]\\tLoss: {loss_list[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title('Hybrid NN Training Convergence')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('CrossEntropy Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "with torch.no_grad(): # Don't compute gradients\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader): # Loop over the test set\n",
    "        output = model(data)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = loss_func(output, target)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    print('Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n",
    "        sum(total_loss) / len(total_loss),\n",
    "        correct / len(test_loader) * 100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS JUST TO VISUALIZE THE PREDICTIONS\n",
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = model(data)\n",
    "        print(output)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title('Predicted {}'.format(pred.item()))\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dark mode\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "  html{filter:invert(1)}\n",
    "  div.prompt{opacity: 0.5;}\n",
    "  .btn-default{border-color: transparent;}\n",
    "  #header-container{display:none !important;}\n",
    "  div.cell.selected, div.cell.selected.jupyter-soft-selected{border-color: transparent;}\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
