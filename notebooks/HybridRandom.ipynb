{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current problem we're facing is\n",
    "\n",
    "> The simulations run too slow. An average iteration in `Net` below for a classical layer takes ~10ns whereas for a quantum layer it takes ~0.5s, that too parallelized to (8/16 threads)\n",
    "\n",
    "This is most likely since most of Qiskit is written in Python and not in C-family/Fortran top to bottom. In fact the only C layer in all of Qiskit is the `Aer` package which is why it takes 0.5s and not something like 15s.\n",
    "\n",
    "There is no way I know of so far of circumventing this problem. the following are a few considerations:\n",
    "- Using **TorchQuantum**: TorchQuantum does not support Apple Silicon installations (i am using an M2 laptop). The issue is tracked [here](https://github.com/mit-han-lab/torchquantum/issues/98)\n",
    "- Use the **GPU**: The current Benchmark for M2 CPU is faster than the Colab GPU\n",
    "- Using **Apple Silicon** to its full extent: While pytorch is already ready for it, Qiskit is not. The issue is tracked [here](https://github.com/Qiskit/qiskit-aer/issues/1762). With full Apple Silicon support, we can use the M2 to basically as much power as the same order of magnitude as Titan\n",
    "- Using a **Different Algorithm**: See [QCNN.ipynb](./QCNN.ipynb)\n",
    "- Using **Runtime Primitives**: The `EstimatorQNN` class actually returns the energy levels measured in various ways. The circuit is not learning when using those/I don't know how to use it (since I can't find a lot of examples online)\n",
    "- Using `TorchConnector`: There is no speed/learning benefit. Under the hood it uses the same `EstimatorQNN` class\n",
    "\n",
    "### The situation\n",
    "We know for a fact this model works because under various configurations we are seeing learning happening. For $(0,1)$ case it is almost a perfect classifier. For $(0,1,3,6)$ case for small training size it is random but as the trainset becomes larger it starts becoming more and more better than random (I was able to reach ~40%, perfectly random is ~25%)\n",
    "\n",
    "It stands to reason if Qiskit were faster we would see the same thing happening for the full MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from qiskit_algorithms.gradients import ParamShiftEstimatorGradient\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_aer.primitives import Estimator\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit import QuantumCircuit\n",
    "from time import time\n",
    "\n",
    "from utils import gtt\n",
    "\n",
    "n_train = 100\n",
    "n_test = 10\n",
    "qubits = 13\n",
    "shots = 1024\n",
    "\n",
    "# train_loader, test_loader = gtt(n_train, [i for i in range(10)])\n",
    "train_loader, test_loader = gtt(n_train, [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NC4 from I, X, Y, Z\n",
    "paulis = [\"I\", \"X\", \"Y\", \"Z\"]\n",
    "def select(n):\n",
    "    if n == 0:\n",
    "        return [\"\"]\n",
    "    if n == 1:\n",
    "        return [\"I\", \"X\", \"Y\", \"Z\"]\n",
    "    return [a+b for a in select(n-1) for b in paulis]\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, qubits):\n",
    "        self.backend = Estimator()\n",
    "        possible = select(5 if qubits > 5 else qubits)\n",
    "        # TODO: select properly\n",
    "        possible = np.random.choice(possible, qubits)\n",
    "        if qubits > 5:\n",
    "            possible = [\"I\"*(qubits-5)+p for p in possible]\n",
    "        self.obs = [self.op(p) for p in possible]\n",
    "\n",
    "        circ = QuantumCircuit(qubits)\n",
    "        for i in range(qubits):\n",
    "            circ.h(i)\n",
    "            t = Parameter('t'+str(i))\n",
    "            circ.cx(i, (i+1)%qubits)\n",
    "            circ.rx(t, i)\n",
    "        circ.measure_all()\n",
    "        self.circuit = circ\n",
    "\n",
    "    def op(self, p):\n",
    "        return SparsePauliOp.from_list([(p, 1)])\n",
    "\n",
    "    def run(self, inputs): # Runs a circuit\n",
    "        qc = self.circuit.assign_parameters(inputs)\n",
    "        result = self.backend.run(\n",
    "            [qc]*len(self.obs),\n",
    "            self.obs,\n",
    "            shots=shots,\n",
    "        ).result()\n",
    "        return torch.tensor(result.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridFunction(Function):\n",
    "    def forward(ctx, input, quantum_circuit):\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.quantum_circuit = quantum_circuit\n",
    "        ctx.shift = np.pi / 2\n",
    "\n",
    "        results = [];\n",
    "        for i in range(len(input)):\n",
    "            expz = ctx.quantum_circuit.run(input[i].tolist())\n",
    "            results.append(torch.tensor(np.array([expz])))\n",
    "\n",
    "        return torch.stack(results).squeeze(1)\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs = np.array(ctx.saved_tensors[0])\n",
    "\n",
    "        shiftr = inputs + np.ones(inputs.shape) * ctx.shift\n",
    "        shiftl = inputs - np.ones(inputs.shape) * ctx.shift\n",
    "\n",
    "        gradients = []\n",
    "        for i in range(len(inputs)):\n",
    "            expr = ctx.quantum_circuit.run(shiftr[i])\n",
    "            expl = ctx.quantum_circuit.run(shiftl[i])\n",
    "\n",
    "            gradient = torch.tensor(expr - expl)\n",
    "            gradients.append(gradient)\n",
    "        # endfor\n",
    "\n",
    "        gradients = torch.stack(gradients).squeeze(1)\n",
    "        return gradients * grad_output.float(), None, None\n",
    "\n",
    "class Hybrid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Hybrid, self).__init__()\n",
    "        self.quantum_circuit = Generator(13)\n",
    "    # end\n",
    "\n",
    "    def forward(self, input):\n",
    "        return HybridFunction.apply(input, self.quantum_circuit)\n",
    "    # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # the actual neural net as mentioned in the tutorial\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=4)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=4)\n",
    "        out_conv1 = F.max_pool2d(self.conv1(torch.rand(1,1,28,28)), 2);\n",
    "        out_conv2 = F.max_pool2d(self.conv2(out_conv1), 2)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(out_conv2.view(1,-1).shape[1], qubits)\n",
    "        self.hybrid = Hybrid()\n",
    "        # TODO: pass real data and not random\n",
    "        out_hybrid = self.hybrid(torch.rand(qubits,qubits))\n",
    "        self.fc2 = nn.Linear(out_hybrid.shape[1], 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x) # We don't relu this to prevent learning, we pass as-is to QC\n",
    "        x = self.hybrid(x).type(torch.FloatTensor)\n",
    "        x = self.fc2(x)\n",
    "        return x;\n",
    "\n",
    "model = Net();\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20 # for now 20\n",
    "loss_list = [3]\n",
    "\n",
    "model.train()\n",
    "\n",
    "targets = []\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    times = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        now = time()\n",
    "        # optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        targets.append(target)\n",
    "\n",
    "        loss = loss_func(output, target) # Loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "        times.append(time() - now)\n",
    "\n",
    "    print(f\"Avg Itr Time: {np.round(np.average(times),1)}s x {len(times)} itrs = {np.round(np.sum(times)/60,1)}min\")\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "\n",
    "    diff = np.abs(loss_list[-1] - loss_list[-2]) /loss_list[-1];\n",
    "    if diff <= 0.001: # Early stopping criterial loss diff = 0.1%\n",
    "        break;\n",
    "\n",
    "    print(f'Training [{100. * (epoch + 1) / epochs:.0f}%]\\tLoss: {loss_list[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "with torch.no_grad(): # Don't compute gradients\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader): # Loop over the test set\n",
    "        output = model(data)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = loss_func(output, target)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    print('Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n",
    "        sum(total_loss) / len(total_loss),\n",
    "        correct / len(test_loader) * 100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = model(data)\n",
    "        print(output)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title('Predicted {}'.format(pred.item()))\n",
    "\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
